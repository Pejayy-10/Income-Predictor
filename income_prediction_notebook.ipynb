{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income Prediction Model - Data Science Pipeline\n",
    "\n",
    "This notebook demonstrates the complete data science pipeline for the Income Prediction project. We'll go through the following steps:\n",
    "\n",
    "1. Data Loading and Exploration\n",
    "2. Data Cleaning and Preprocessing\n",
    "3. Feature Engineering\n",
    "4. Feature Selection\n",
    "5. Model Training and Evaluation\n",
    "6. Model Saving for Production Use\n",
    "\n",
    "The goal is to predict whether a person's income exceeds $50K per year based on census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import pickle\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define column names since the dataset doesn't include them\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'sex', \n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('adult.data', names=column_names, sep=', ', engine='python')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check the distribution of the target variable\n",
    "income_counts = df['income'].value_counts()\n",
    "print(income_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='income', data=df)\n",
    "plt.title('Income Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage\n",
    "print(f\"Percentage of >50K: {income_counts[' >50K'] / len(df) * 100:.2f}%\")\n",
    "print(f\"Percentage of <=50K: {income_counts[' <=50K'] / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values (in this dataset, missing values are marked as '?')\n",
    "print(\"Missing values (marked as '?'):\")\n",
    "for column in df.columns:\n",
    "    missing_count = (df[column] == ' ?').sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"{column}: {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Replace '?' with NaN and then handle missing values\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].replace(' ?', np.nan)\n",
    "\n",
    "# Handle missing values - for simplicity, we'll drop rows with missing values\n",
    "# In a real project, you might want to use imputation instead\n",
    "df_cleaned = df.dropna()\n",
    "print(f\"Shape after removing missing values: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for duplicates\n",
    "duplicates = df_cleaned.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(\"Duplicates removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Strip leading/trailing whitespace from string columns\n",
    "for column in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    df_cleaned[column] = df_cleaned[column].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Age distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df_cleaned, x='age', hue='income', bins=30, kde=True)\n",
    "plt.title('Age Distribution by Income')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Education level and income\n",
    "plt.figure(figsize=(12, 6))\n",
    "education_order = df_cleaned.groupby('education')['education_num'].mean().sort_values().index\n",
    "sns.countplot(data=df_cleaned, y='education', hue='income', order=education_order)\n",
    "plt.title('Education Level vs Income')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Education Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hours per week distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_cleaned, x='income', y='hours_per_week')\n",
    "plt.title('Hours per Week by Income')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Hours per Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Occupation and income\n",
    "plt.figure(figsize=(12, 8))\n",
    "occupation_income = df_cleaned.groupby('occupation')['income'].apply(lambda x: (x == '>50K').mean()).sort_values()\n",
    "sns.barplot(x=occupation_income.values, y=occupation_income.index)\n",
    "plt.title('Percentage of >50K Income by Occupation')\n",
    "plt.xlabel('Percentage with >50K Income')\n",
    "plt.ylabel('Occupation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Correlation between numerical features\n",
    "numerical_cols = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_cleaned[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create age groups\n",
    "df_cleaned['age_group'] = pd.cut(\n",
    "    df_cleaned['age'], \n",
    "    bins=[0, 25, 35, 45, 55, 65, 100], \n",
    "    labels=['<25', '25-35', '35-45', '45-55', '55-65', '65+']\n",
    ")\n",
    "\n",
    "# Create hours worked category\n",
    "df_cleaned['work_intensity'] = pd.cut(\n",
    "    df_cleaned['hours_per_week'], \n",
    "    bins=[0, 20, 40, 60, 100], \n",
    "    labels=['Part-time', 'Full-time', 'Overtime', 'Workaholic']\n",
    ")\n",
    "\n",
    "# Create a feature for education level (simplified)\n",
    "education_map = {\n",
    "    'Preschool': 'Low',\n",
    "    '1st-4th': 'Low',\n",
    "    '5th-6th': 'Low',\n",
    "    '7th-8th': 'Low',\n",
    "    '9th': 'Low',\n",
    "    '10th': 'Medium',\n",
    "    '11th': 'Medium',\n",
    "    '12th': 'Medium',\n",
    "    'HS-grad': 'Medium',\n",
    "    'Some-college': 'Medium',\n",
    "    'Assoc-voc': 'High',\n",
    "    'Assoc-acdm': 'High',\n",
    "    'Bachelors': 'High',\n",
    "    'Masters': 'Very High',\n",
    "    'Prof-school': 'Very High',\n",
    "    'Doctorate': 'Very High'\n",
    "}\n",
    "df_cleaned['education_level'] = df_cleaned['education'].map(education_map)\n",
    "\n",
    "# Check the new features\n",
    "df_cleaned[['age', 'age_group', 'hours_per_week', 'work_intensity', 'education', 'education_level']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert the target to binary (1 for >50K, 0 for <=50K)\n",
    "df_cleaned['income'] = df_cleaned['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "print(f\"Target variable distribution:\\n{df_cleaned['income'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df_cleaned.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove target from features\n",
    "if 'income' in numerical_cols:\n",
    "    numerical_cols.remove('income')\n",
    "\n",
    "# Select K best features\n",
    "X = df_cleaned.drop(columns=['income'])\n",
    "y = df_cleaned['income']\n",
    "\n",
    "# Using SelectKBest for numerical features\n",
    "selector = SelectKBest(f_classif, k=min(10, len(numerical_cols)))\n",
    "selector.fit(df_cleaned[numerical_cols], y)\n",
    "selected_numerical = [numerical_cols[i] for i in selector.get_support(indices=True)]\n",
    "print(f\"Selected numerical features: {selected_numerical}\")\n",
    "\n",
    "# For categorical features, we'll select based on domain knowledge\n",
    "selected_categorical = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "print(f\"Selected categorical features: {selected_categorical}\")\n",
    "\n",
    "selected_features = selected_numerical + selected_categorical\n",
    "print(f\"Total selected features: {len(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Processing and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data\n",
    "X = df_cleaned[selected_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create preprocessing pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, selected_numerical),\n",
    "        ('cat', categorical_transformer, selected_categorical)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create pipeline with preprocessing and model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Training {name}...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'pipeline': pipeline,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    # Save the best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = pipeline\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} with accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confusion Matrix for the best model\n",
    "y_pred = results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['<=50K', '>50K'],\n",
    "            yticklabels=['<=50K', '>50K'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ROC Curve for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    pipeline = result['pipeline']\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Different Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance for Random Forest\n",
    "if 'Random Forest' in results:\n",
    "    rf_pipeline = results['Random Forest']['pipeline']\n",
    "    rf_model = rf_pipeline.named_steps['model']\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    preprocessor = rf_pipeline.named_steps['preprocessor']\n",
    "    cat_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(selected_categorical)\n",
    "    feature_names = np.concatenate([selected_numerical, cat_features])\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('Feature Importances - Random Forest')\n",
    "    plt.bar(range(min(20, len(importances))), importances[indices[:20]], align='center')\n",
    "    plt.xticks(range(min(20, len(importances))), feature_names[indices[:20]], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the best model\n",
    "if best_accuracy >= 0.8:  # Check if accuracy meets requirement\n",
    "    # Save the model using pickle\n",
    "    with open('income_prediction_model.pkl', 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "    print(\"Model saved successfully with accuracy above 80%!\")\n",
    "    \n",
    "    # Also save the feature list for future use\n",
    "    with open('selected_features.pkl', 'wb') as file:\n",
    "        pickle.dump(selected_features, file)\n",
    "    print(\"Selected features saved.\")\n",
    "    \n",
    "    # Save the preprocessor for future use\n",
    "    with open('preprocessor.pkl', 'wb') as file:\n",
    "        pickle.dump(preprocessor, file)\n",
    "    print(\"Preprocessor saved.\")\n",
    "else:\n",
    "    print(f\"Model accuracy ({best_accuracy:.4f}) is below 80%. Consider improving the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions on a few test samples\n",
    "sample_indices = np.random.choice(len(X_test), min(5, len(X_test)), replace=False)\n",
    "samples = X_test.iloc[sample_indices]\n",
    "true_labels = y_test.iloc[sample_indices]\n",
    "predictions = best_model.predict(samples)\n",
    "probabilities = best_model.predict_proba(samples)[:, 1]\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "for i, (idx, sample) in enumerate(samples.iterrows()):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    for feature, value in sample.items():\n",
    "        print(f\"  {feature}: {value}\")\n",
    "    print(f\"  True label: {'Income >50K' if true_labels.iloc[i] == 1 else 'Income <=50K'}\")\n",
    "    print(f\"  Predicted label: {'Income >50K' if predictions[i] == 1 else 'Income <=50K'}\")\n",
    "    print(f\"  Confidence: {probabilities[i]*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built a complete data science pipeline for income prediction:\n",
    "\n",
    "1. We loaded and explored the Census Income dataset\n",
    "2. We cleaned the data by handling missing values and duplicates\n",
    "3. We performed feature engineering to create new informative features\n",
    "4. We selected the most relevant features for our model\n",
    "5. We trained and compared multiple machine learning models\n",
    "6. We evaluated the models using various metrics\n",
    "7. We saved the best model for use in our Django application\n",
    "\n",
    "The best model achieved good accuracy and can now be used to predict whether a person's income exceeds $50K based on their demographic and employment information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
